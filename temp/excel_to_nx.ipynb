{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************************\n",
    "# Import Packages\n",
    "# ************************\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from IPython.display import Image, display, Markdown, HTML\n",
    "import copy\n",
    "from pprint import pprint\n",
    "import helpers\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************************\n",
    "# User Defined Parameters\n",
    "# ************************\n",
    "infile = \"../pyCIMS_model_description.xlsm\"\n",
    "NODE_COL = \"Node\"\n",
    "TYPE_COL = \"Demand?\"\n",
    "MODEL_SHEET = \"Model\"\n",
    "EXTRA_COL = 'Demand?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************************\n",
    "# Read in the data\n",
    "# ************************\n",
    "\n",
    "# Read model_description from excel\n",
    "mxl = pd.read_excel(infile, sheet_name=None, header=1)\n",
    "\n",
    "# Read the model sheet into a dataframe\n",
    "model_df = mxl[MODEL_SHEET].replace({pd.np.nan: None})\n",
    "\n",
    "# Adjust index to correspond to Excel line numbers\n",
    "model_df.index += 3  #(+1: 0 vs 1 origin, +1: header skip, +1: column headers)\n",
    "\n",
    "# Convert all column names to strings (years were ints)\n",
    "model_df.columns = [str(c) for c in model_df.columns]\n",
    "\n",
    "# Find the columns, separated by whether they are year columns or not\n",
    "node_cols, year_cols = helpers.get_node_cols(model_df, extra_col=EXTRA_COL)\n",
    "all_cols = np.concatenate((node_cols, year_cols))\n",
    "\n",
    "# Create the model dataframe\n",
    "mdf = model_df.loc[1:,all_cols] # drop irrelevant columns and skip first, empty row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ************************\n",
    "# # Extract Node Dataframes\n",
    "# # ************************\n",
    "# # determine, row ranges for each node def, based on non-empty Node field\n",
    "# node_rows = mdf.Node[~mdf.Node.isnull()] # does not work if node names have been filled in\n",
    "# node_rows.index.name = \"Row Number\"\n",
    "# last_row = mdf.index[-1]\n",
    "# node_start_ends = zip(node_rows.index,\n",
    "#                       node_rows.index[1:].tolist() + [last_row])\n",
    "\n",
    "# # extract Node DataFrames, at this point still including Technologies\n",
    "# node_dfs = {}\n",
    "# non_node_cols = mdf.columns != NODE_COL\n",
    "# for s, e in node_start_ends:\n",
    "#     node_name = mdf.Node[s]\n",
    "#     node_df = mdf.loc[s+1:e-1]\n",
    "#     node_df = node_df.loc[helpers.non_empty_rows(node_df), non_node_cols]\n",
    "#     node_dfs[node_name] = node_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************************\n",
    "# Extract Node Dataframes\n",
    "# ************************\n",
    "# determine, row ranges for each node def, based on non-empty Node field\n",
    "node_rows = mdf.Node[~mdf.Node.isnull()] # does not work if node names have been filled in\n",
    "node_rows.index.name = \"Row Number\"\n",
    "last_row = mdf.index[-1]\n",
    "node_start_ends = zip(node_rows.index,\n",
    "                      node_rows.index[1:].tolist() + [last_row])\n",
    "\n",
    "# extract Node DataFrames, at this point still including Technologies\n",
    "node_dfs = {}\n",
    "non_node_cols = mdf.columns != NODE_COL\n",
    "for s, e in node_start_ends:\n",
    "#     node_name = mdf.Node[s]\n",
    "    node_df = mdf.loc[s+1:e-1]\n",
    "    node_df = node_df.loc[helpers.non_empty_rows(node_df), non_node_cols]\n",
    "    \n",
    "    try:\n",
    "        node_name = list(node_df[node_df['Parameter']=='Service provided']['Branch'])[0]\n",
    "    except IndexError:\n",
    "        continue   \n",
    "\n",
    "    node_dfs[node_name] = node_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************************\n",
    "# Extract Tech Dataframes\n",
    "# ************************\n",
    "# Extract tech dfs from node df's and rewrite node df without techs\n",
    "tech_dfs = {}\n",
    "for nn, ndf in node_dfs.items():\n",
    "    if any(ndf.Parameter.isin([\"Technology\", \"Service\"])):  # Technologies can also be called Services\n",
    "        tdfs = {}\n",
    "        first_row, last_row = ndf.index[0], ndf.index[-1]\n",
    "        tech_rows = ndf.loc[ndf.Parameter.isin([\"Technology\", \"Service\"])].index\n",
    "        for trs, tre in zip(tech_rows, tech_rows[1:].tolist()+[last_row]):\n",
    "            tech_df = ndf.loc[trs:tre-1]\n",
    "            tech_name = tech_df.iloc[0].Value\n",
    "            tdfs[tech_name] = tech_df\n",
    "        tech_dfs[nn] = tdfs\n",
    "        node_dfs[nn] = ndf.loc[:tech_rows[0]-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************************\n",
    "# Display Model Dataframes\n",
    "# ************************\n",
    "# display content of entire Model dataframe as separate df's\n",
    "# for nn, ndf in node_dfs.items():\n",
    "#     display(Markdown(\"Node: **{}**\".format(nn)))\n",
    "#     helpers.display_df(ndf)\n",
    "#     if nn in tech_dfs:\n",
    "#         for tech_name, tdf in tech_dfs[nn].items():\n",
    "#             display(Markdown(\"Node / Technology: **{} / {}**\".format(nn, tech_name)))\n",
    "#             helpers.display_df(tdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jillian's Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************************\n",
    "# Functions\n",
    "# ************************\n",
    "def add_node_data(g, current_node,):\n",
    "    \"\"\"\n",
    "    @param g: NetworkX DiGraph. \n",
    "    @param current_node: The \n",
    "    @return: None. Modified the graph g instead. \n",
    "    Given a graph g and node current_node add current_node to g, along with all of its associated data.\"\"\"\n",
    "\n",
    "    # Copy the current node dataframe\n",
    "    current_node_df = copy.deepcopy(node_dfs[current_node])\n",
    "\n",
    "    # 1. we are going to create a node in the graph\n",
    "    g.add_node(current_node)\n",
    "\n",
    "    # 2. We will store the Supply/Demand Type of node. This is a special case. \n",
    "    typ = list(current_node_df[TYPE_COL])[0]\n",
    "    g.node[current_node]['type'] = typ.lower() if typ else 'standard'\n",
    "    # Drop Demand column\n",
    "    current_node_df = current_node_df.drop(TYPE_COL, axis=1)\n",
    "\n",
    "    # 3. We will store the Competition Type of the node at the node level. This is another special case. \n",
    "    comp_list = list(current_node_df[current_node_df['Parameter']=='Competition type']['Value'])\n",
    "    if len(set(comp_list)) == 1: \n",
    "        comp_type = comp_list[0]\n",
    "        g.node[current_node]['competition_type'] = comp_type.lower()\n",
    "    elif len(set(comp_list)) > 1:\n",
    "        print(\"TOO MANY COMPETITION TYPES\")\n",
    "    # Get rid of competition type row\n",
    "    current_node_df = current_node_df[current_node_df['Parameter']!='Competition type']\n",
    "\n",
    "    # 4. For the remaining rows, group data by year. \n",
    "    # Get Year Columns\n",
    "    years = [c for c in current_node_df.columns if helpers.is_year(c)]\n",
    "\n",
    "    # Get Non-Year Columns\n",
    "    non_years = [c for c in current_node_df.columns if not helpers.is_year(c)]\n",
    "\n",
    "    # For each year: \n",
    "    for y in years:\n",
    "        year_df = current_node_df[non_years + [y]]\n",
    "        year_dict = {}\n",
    "\n",
    "        for parameter, source, branch, unit, value, year_value in zip(*[year_df[c] for c in year_df.columns]):\n",
    "            if parameter in year_dict.keys():\n",
    "                pass\n",
    "            else:\n",
    "                year_dict[parameter] = {}\n",
    "\n",
    "            dct = {'source': source,\n",
    "                   'branch': branch, \n",
    "                   'unit': unit, \n",
    "                   'year_value': year_value}\n",
    "#             # Clean Dict\n",
    "#             clean_dict = {k: v for k, v in dct.items() if v is not None}\n",
    "            \n",
    "            year_dict[parameter][value] = dct\n",
    "            \n",
    "        # Add data to node\n",
    "        g.node[current_node][y] = year_dict\n",
    "    \n",
    "    \n",
    "def add_tech_data(node, tech, tech_df):\n",
    "    # TODO: I think we need to differentiate between Technologies and Services. \n",
    "\n",
    "    t_df = copy.deepcopy(tech_df)\n",
    "    \n",
    "    # 1. Find whether technology is a service or a technology. \n",
    "    #    Then remove the row that indicates this is a service or technology. \n",
    "    service_technology = 'service' if (t_df['Parameter']=='Service').any() else 'technology'\n",
    "    t_df = t_df[~t_df['Parameter'].isin(['Service', 'Technology'])]\n",
    "\n",
    "    # 2. Remove the Demand? column\n",
    "    t_df = t_df.drop('Demand?', axis=1)\n",
    "\n",
    "    # VERY SIMILAR to what we do for nodes. But not quite. Because we don't use the value column anymore\n",
    "    # 4. For the remaining rows, group data by year. \n",
    "    # Get Year Columns\n",
    "    years = [c for c in t_df.columns if helpers.is_year(c)]\n",
    "\n",
    "    # Get Non-Year Columns\n",
    "    non_years = [c for c in t_df.columns if not helpers.is_year(c)]\n",
    "\n",
    "    # For each year: \n",
    "    for y in years:\n",
    "        year_df = t_df[non_years + [y]]\n",
    "        year_dict = {}\n",
    "\n",
    "        for parameter, source, branch, unit, value, year_value in zip(*[year_df[c] for c in year_df.columns]):\n",
    "            dct = {'source': source,\n",
    "                       'branch': branch, \n",
    "                       'unit': unit, \n",
    "                       'year_value': year_value}\n",
    "            \n",
    "            if parameter in year_dict.keys():\n",
    "                if type(year_dict[parameter]) is list:\n",
    "                    year_dict[parameter] = year_dict[parameter].append(dct)\n",
    "                else: \n",
    "                    year_dict[parameter] = [year_dict[parameter], dct] \n",
    "            else:\n",
    "                year_dict[parameter] = dct\n",
    "\n",
    "        # Add technologies key if needed\n",
    "        if not 'technologies' in g.node[node][y].keys():\n",
    "            g.node[node][y]['technologies'] = {}\n",
    "            \n",
    "        # Add the technology specific data for that year\n",
    "        g.node[node][y]['technologies'][tech]= year_dict\n",
    "        \n",
    "def search_nodes(g, search_term):\n",
    "    \"\"\"Search nodes to see if there is one that contains the search term in the final component of its name\"\"\"\n",
    "    def search(name):\n",
    "        components = name.split('.')\n",
    "        last_comp = components[-1]\n",
    "        \n",
    "        return search_term.lower() in last_comp.lower()\n",
    "    \n",
    "    return [n for n in g.nodes if search(n)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************************\n",
    "# Add Node Data\n",
    "# ************************\n",
    "\n",
    "# Create Graph\n",
    "g = nx.DiGraph()\n",
    "\n",
    "# Add each node and its associated data to the Graph\n",
    "for n in node_dfs.keys():\n",
    "    add_node_data(g, n)\n",
    "    print(\"************* {} *************\".format(n))\n",
    "    pprint(g.node[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clothes_node = search_nodes(g, 'clothes')[0]\n",
    "pprint(g.node[clothes_node]['2015'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in tech_dfs:\n",
    "    # Add technologies key to node data\n",
    "    node_techs_dict = {}\n",
    "    for tech in tech_dfs[node]: \n",
    "        add_tech_data(node, tech, tech_dfs[node][tech])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw = search_nodes(g, 'dishwashing')[0]\n",
    "pprint(g.nodes[dw])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Structure Description\n",
    "We have a NetworkX graph structure composed of nodes and edges. Nodes represent services within the model. Edges are directional and represent a request/provide relationship between two services. Edges flow from requesting nodes to providing nodes (down the tree). \n",
    "\n",
    "## Node Data\n",
    "Each node has its own associated data, stored at the node in nested dictionaries. Each node's data dictionary contains 13 keys. 11 of these correspond to years within the simulation and have dictionaries as their values. The other two keys are: \n",
    "* `type`: One of supply, demand, or standard. This specifies whether a node is used during the supply or demand portion of the algorithm. Standard nodes are used in both (they root the tree and have children of both supply and demand types). Supply nodes are used only during supply calculations. Similarly, demand nodes are used only during demand calculations. \n",
    "* `competition_type`: One of 'tech_compete' or 'winner_takes_all'. Specifies the competition type that will be used to calculate supply/demand at that node. Not all nodes will have this attribute. \n",
    "\n",
    "### Year Dictionaries\n",
    "The year dictionaries, contain all the data specific to a given year. These year specific dictionaries contain keys for regular attributes (see below for description) and may contain a `\"technologies\"` key. \n",
    "* regular attributes correspond to non-technology parameters from the model description. Examples include price, service requested, service provided, and heterogeniety. Regular attributes are given a key corresponding to the `Parameter` column from the model description and a value that is a dictionary composed from the `branch`, `source`, `unit` and `value` columns from the model description in addition to the and year-specific values provided in the 2000 ... 2050 columns within the model description. They follow the format below: \n",
    "```\n",
    "'parameter1': {'value1': {'branch': <val>, \n",
    "                          'source': <val>, \n",
    "                          'unit': <val>, \n",
    "                          'year_value': <val>}}\n",
    "```\n",
    "See the output from the cell below to see an example of node data containing regular attributes. For simplicity I have only included the 2020 subdictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alberta = search_nodes(g, 'alberta')[0]\n",
    "pprint(g.node[alberta])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `technologies`: Will only be present in nodes with a 'tech_compete' or 'winner_takes_all' competition type. Once again, the value will be a dictionary of dictionaries, with one dictionary for each technology. Each technologies entry is set up as shown below, with <val> corresponding to the value found in the appropriate year column in the model description. See the next cell's output for an example of a technology dictionary.  \n",
    "```\n",
    "'technology1': {'parameter1' : {'branch': <val>\n",
    "                                'source': <val>, \n",
    "                                'unit': <val>, \n",
    "                                'year_value': <val>}, \n",
    "                'parameter2' : {'branch': <val>\n",
    "                                'source': <val>, \n",
    "                                'unit': <val>, \n",
    "                                'year_value': <val>}}\n",
    "```\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*****************************\")\n",
    "print(\"Electric Baseboard Technology\")\n",
    "print(\"*****************************\")\n",
    "space_heating = search_nodes(g, 'space heating')[0]\n",
    "\n",
    "pprint(g.node[space_heating]['2000']['technologies']['Electric baseboard'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_edges(node, df, g):\n",
    "    # Find edges based on Requester/Provider relationships\n",
    "    # ----------------------------------------------------\n",
    "    # Find all nodes node is requesting services from\n",
    "    providers = df[df['Parameter']=='Service requested']['Branch'].unique()\n",
    "    rp_edges = [(node, p) for p in providers]\n",
    "    g.add_edges_from(rp_edges)\n",
    "    \n",
    "    # Add them to the graph\n",
    "    for e in rp_edges:\n",
    "        try:\n",
    "            types = g.edges[e]['type']\n",
    "            if not 'request_provide' in types:\n",
    "                g.edges[e]['type'] += ['request_provide']\n",
    "        except KeyError:\n",
    "            g.edges[e]['type'] = ['request_provide']\n",
    "            \n",
    "\n",
    "    # Find edges based on branch\n",
    "    # --------------------------\n",
    "    # Find the node's parent\n",
    "    parent = '.'.join(node.split('.')[:-1])\n",
    "    s_edges = []\n",
    "    if parent: \n",
    "        s_edges += [(parent, node)]    \n",
    "    g.add_edges_from(s_edges)\n",
    "        \n",
    "    # Add them to the graph\n",
    "    for e in s_edges:\n",
    "        try:\n",
    "            types = g.edges[e]['type']\n",
    "            if not 'structure' in types:\n",
    "                g.edges[e]['type'] += ['structure']\n",
    "        except KeyError:\n",
    "            g.edges[e]['type'] = ['structure']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in node_dfs:\n",
    "    add_edges(node, node_dfs[node], g)\n",
    "    \n",
    "for node in tech_dfs:\n",
    "    for tech in tech_dfs[node]:\n",
    "        add_edges(node, tech_dfs[node][tech], g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "pos = graphviz_layout(g, prog='dot')\n",
    "plt.figure(figsize=(20,12))\n",
    "nx.draw(g,pos,with_labels=True, font_size=7, node_color='gainsboro', font_color='tomato')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the Supply Sub-graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the demand sub-graph\n",
    "# d_nodes = [n for n, a in g.nodes(data=True) if a['type'] in ('demand', 'standard')]\n",
    "d_nodes = []\n",
    "for n, a in g.nodes(data=True):\n",
    "    try:\n",
    "        a['type']\n",
    "        if a['type'] in ['supply', 'standard']:\n",
    "            d_nodes.append(n)\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "g_demand = g.subgraph(d_nodes).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = graphviz_layout(g_demand, prog='dot')\n",
    "plt.figure(figsize=(20,12))\n",
    "nx.draw(g_demand, pos,with_labels=True, font_size=7, node_color='gainsboro', font_color='tomato')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"graph.pickle\",\"wb\")\n",
    "pickle.dump(g, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.nodes['Canada']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
